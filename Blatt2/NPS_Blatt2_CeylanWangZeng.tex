\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}

\usepackage{geometry}
\geometry{a4paper,left=3cm,right=3cm,top=2.5cm,bottom=2.5cm}

\usepackage{multicol}

\renewcommand{\baselinestretch}{1.45} 

\title{Non-Paramatric Statistics Sheet 2}
\author{Osman Ceylan, Jiahui Wang, Zhuoyao Zeng}
\date{\today}

\begin{document}
\maketitle
\section*{Exercise 1.1}
Let $(X, \mathcal{A})$ be a measurable space, $Y := \{-1,1\}$, and $P$ be a distribution on $X \times Y$. Define $\eta: X \rightarrow [0,1]$ by $\eta(x) := P(\{1\}|x)$ for all $x \in X$.\vspace{0.7em}\\
\textsl{i)} Use $\eta$ to determine the Bayes risk and all Bayes decision functions for the binary classification loss $L_\text{class}:Y \rightarrow [0,\infty )$.\vspace{0.5em}\\
\textsl{Solution:} \\
Basic assumptions:\\
We have regular conditional probability $P(\cdot|x)$ met the following conditions:\\ 
\indent 1) $P(\cdot|x):\{-1,1\} \rightarrow [0,1]$ is a probability measure \\
\indent 2) $x \mapsto P(B|x)$ is measurable for $B = \{1\}$ or $B = \{-1\}$ \\
\indent 3) $P(A \times B) = \displaystyle{ \int_{A} P(B|x)\, \text{d}P_X  }$\\
 \\
Since the prediction $t \in \{0,1,-1\}$ or actually should it be $t \in \mathbb{R}$, we define $\tilde{t}:= \text{sign}(t)$ with sign$(0) = 1$ to simplify the expression. \\
Now we look for our $C^*_{L,p}(x):= \inf_{\tilde{t}\in \{-1,1\}}\{\displaystyle{ \int_{Y} L_\text{class}(y,\tilde{t})\, P(\text{d}y|x)} \}$.  
\begin{align*}
\displaystyle{ \int_{Y} L_\text{class}(y,\tilde{t})\, P(\text{d}y|x) }
 & = \displaystyle{ \int_{Y} \mathbbm{1}_{(-\infty,0]}(y\cdot\tilde{t}) P(\text{d}y|x)} \\
 & = 1 \cdot P(y\cdot\tilde{t} \leq 0 | x) =  \begin{cases} P(\{y=1\}|x) = \eta(x), &\tilde{t}(x) = -1 \cr  P(\{y=-1\}|x) = 1- \eta(x), &\tilde{t}(x) = 1 \end{cases}.
\end{align*}
To minimize our integral in $\tilde{t}$ for a given $x$, we just need to compare $\eta(x)$ with $1 - \eta(x)$. More precisely, we make the following choices for an $x\in X$:\\
When $\eta(x) \geq 1-\eta(x) \; \Leftrightarrow \; \eta(x) \geq \frac{1}{2}$, we choose $\tilde{t}(x) := 1$,\\
When $\eta(x) < 1-\eta(x) \;\Leftrightarrow\; \eta(x) < \frac{1}{2}$, we choose $\tilde{t}(x) := -1$.\\
Now consider all $x\in X$. The target function $t^*$, according to the algorithm above, should be:
\begin{align*}
t^*(x) = \begin{cases} 1, &\eta(x) \geq \frac{1}{2} \cr -1, &\eta(x) < \frac{1}{2} \end{cases} 
\indent = \; \mathbbm{1}_{\{\eta \geq \frac{1}{2}\}} - \mathbbm{1}_{\{\eta < \frac{1}{2}\}}.
\end{align*}
$\Rightarrow \;\; C^*_{L,p}(x) = \displaystyle{ \int_{Y} \mathbbm{1}_{(0,\infty]}(y\cdot t^*(x)) P(\text{d}y|x)} =  1 \cdot P(y\cdot t^*(x) \leq 0 | x) = \min \{ \eta(x), 1-\eta(x) \}\;\; P_X\text{-almost surely}.$\vspace{0.05em}\\
According to (1.2.9), all functions satisfying the equation above are Bayes decision functions $P_X\text{-almost surely}.$.\\
Now we compute the Bayes Risk:\\
$R^*_{L,P} =  \displaystyle{ \int_{X} C^*_{L,p}(x)\, \text{d}P_X(x) = \int_{\{\eta(x)\geq \frac{1}{2}\}}(1-\eta(x))\, \text{d}P_X(x) + \int_{\{\eta(x) < \frac{1}{2}\}}\eta(x)\, \text{d}P_X(x) } $.\\
 \\
\textsl{ii)} Given a so-called weight parameter $\alpha \in (0,1)$ and consider the $\alpha$-weighted binary classification loss $L_{\text{class},\alpha} : Y \times \mathbb{R} \rightarrow [0,\infty)$ defined by:
\begin{align*}
L_{\text{class},\alpha} = \begin{cases} 1-\alpha, & y=1 \land t<0 \cr \alpha, & y=-1 \land t\geq 0 \cr 0, &\text{otherwise}. \end{cases} .
\end{align*}
Determine the corresponding Bayes risk and Bayes decision functions and compare your findings to part i).
\vspace{0.5em}\\
\textsl{Solution:} \\
Analogously to part i), we want to minimize the average loss in $t$ according to $P($d$y|x)$ for every given $x \in X$. In the second case, 
\begin{align*}
\displaystyle{ \int_{Y} L_{\text{class},\alpha}(y,\tilde{t})\, P(\text{d}y|x) }
 & = \begin{cases} (1-\alpha)\cdot P(\{y=1\}|x) = (1-\alpha)\cdot \eta(x), &\tilde{t}(x) = -1 \cr  \alpha \cdot P(\{y=-1\}|x) = \alpha \cdot (1- \eta(x)), &\tilde{t}(x) = 1. \end{cases}.
\end{align*}
And we make the following choice:\\
When $(1-\alpha)\eta(x) \geq \alpha(1-\eta(x)) \Leftrightarrow \eta(x) \geq \alpha$, we choose $\tilde{t}(x) = 1$,\\
When $(1-\alpha)\eta(x) < \alpha(1-\eta(x)) \Leftrightarrow \eta(x) < \alpha$, we choose $\tilde{t}(x) = -1$,\\
Now consider all $x\in X$. The target function $t^*$, according to the algorithm above, should be:
\begin{align*}
t^*(x) = \begin{cases} 1, &\eta(x) \geq \alpha \cr -1, &\eta(x) < \alpha \end{cases} 
\indent = \; \mathbbm{1}_{\{\eta \geq \alpha\}} - \mathbbm{1}_{\{\eta < \alpha\}}.
\end{align*}
$\Rightarrow \;\; C^*_{L,p}(x) = \displaystyle{ \int_{Y}  L_{\text{class},\alpha}(y,t^*)\,  P(\text{d}y|x)} \;=\; \min \{ (1-\alpha)\eta(x), \alpha(1-\eta(x)) \}\;\; P_X\text{-almost surely}.$\\
\vspace*{-1em}\\
According to (1.2.8) all functions satisfying the equation of integral above are  Bayes decision functions.\\
Now we compute the Bayes Risk:\\
\vspace*{-1em}\\
$R^*_{L,P} =  \displaystyle{ \int_{X} C^*_{L,p}(x)\, \text{d}P_X = \int_{\{\eta(x)\geq \alpha\}}\alpha(1-\eta(x))\, \text{d}P_X + \int_{\{\eta(x) < \alpha\}} (1-\alpha)\eta(x)\, \text{d}P_X } $.\\
 \\

\newpage

\section*{Exercise 1.4}
Let $(X,\mathcal{A})$ be a measurable space, $\mu$ be a $\sigma$-finite measure on $X$, and $h_{-1}, h_1:X \rightarrow [0,\infty)$ be measurable with 
\begin{align*}
\displaystyle{ \int_{X} h_{-1} \, \text{d}\mu = \int_{X} h_{1} \, \text{d}\mu = 1. } \tag{1.5.1}
\end{align*}
For a given $p\in [0,1]$ and $Y:= \{ -1,1 \}$ consider the distribution $P$ on $X \times Y$ that is uniquely determined by 
\begin{align*}
P(\{ y \} \times A) := 
 & \begin{cases} 
 (1-p)\displaystyle{\int_A h_{-1} \, \text{d}\mu }, & y = -1 \cr  
 p \displaystyle{\int_A h_{1} \, \text{d}\mu }, &y = 1 
\end{cases}. \tag{1.5.2}
\end{align*}
for all $y\in Y$ and $A\in \mathcal{A}$.\\
\\
\textsl{i)} Determine the marginal distribution $P_X$ and the function $\eta:X\rightarrow [0,1]$ that is given by $\eta(x):=P(\{ 1 \}| x)$ for all $x\in X$. \vspace{0.5em}\\
\textsl{Solution:} \\
Consider an arbitrary $A\in\mathcal{A}$. \\
Observe that $Y\times A = \{ -1,1 \} \times A = \{-1\} \times A \ \dot{\cup} \ \{1\} \times A $. \\
With $\sigma$-additivity of measure we have: \\
\indent $P_X(A) = P(Y\times A) =  P( \{-1\} \times A )+P( \{1\} \times A) $ \\
\indent \hspace*{3.1cm}$= \displaystyle{ (1-p)\int_A h_{-1} \,\text{d}\mu + p\int_A h_{1}\, \text{d}\mu = \int_A (1-p)h_{-1}+p\, h_{1}\, \text{d}\mu } $.\\
\vspace*{-1em} \\
Also observe that $f:=(1-p)h_{-1}+p\, h_{1}$ is a positive function. \hspace{4.8cm}$ (\#)$\\
$\Rightarrow \ f$ is the density of $P_X$ with respect to $\mu$ . \hspace{7.6cm}$ (*)$ \\
For $\eta$ we consider the quantity $P(\{ 1\} \times A)$ along with the equality 1.2.3 from lecture slides:\\
\vspace*{-1em} \\
\indent $P(\{ 1\} \times A) \overset{1.2.3}{=} \displaystyle{ \int_{A} P({1}| x)\, \text{d}P_X(x) \overset{(*)}{=} \int_{A} P({1}| x)\, f(x)\, \text{d}\mu(x)  }  $.\vspace*{0.3em}\\
Because of (1.5.2) and $\eta=P(\{ 1\}|x)$ we then have: $\displaystyle{ \int_{A} p\,h_1\, \text{d}\mu =\int_A \eta \, f \, \text{d}\mu  }$.\vspace*{0.1em}\\
Particularly it holds that $\displaystyle{ \int_{N} p\,h_1\, \text{d}\mu =\int_N \eta \, f \, \text{d}\mu  }$ with $N:=\{ x\in X\, | \, (p\,h_1)(x) \neq (\eta \, f)(x)    \}$.\vspace*{0.7em}\\
$\Rightarrow \ $ For $\mu$-almost every $x\in X$ we have $\displaystyle{ p\ h_1 = \eta \, f } $. \\
\vspace*{-1em} \\
$\overset{(\#)}{\Rightarrow} \ $ For $\mu$-almost every $x\in X$ it holds that $\displaystyle{ \eta = \frac{p\, h_{1}}{f} = \frac{p \, h_1}{(1-p)\,h_{-1} + p\, h_1} }$.\\
$(*)$ says that $P_X \ll \mu \ \Rightarrow \ $ For $P_X$-almost every $x\in X$ it holds that $\displaystyle{ \eta = \frac{p \, h_1}{(1-p)\,h_{-1} + p\, h_1} }$.\\
\\
\textsl{ii)} Show that for all distributions $P$ on $X\times Y$ there exists $\mu, p$, and measurable $h_{-1}, h_1: X \rightarrow [0,\infty)$ with (1.5.1) such that (1.5.2) holds. \vspace{0.5em}\\
\textsl{Solution:} \\
$Y=\{-1,1\}$ is a closed subset of $\mathbb{R} \ $\\
$\Rightarrow \ $regular conditional probability $P(\cdot | \cdot )$ with respect to $P$ exists.  \\
Define $\eta(x):=P(\{1\}|x)$ which is measurable per Definition of $P(\cdot | \cdot )$. \\
Consider $p:=P(\{1\} \times X) \in [0,1]$ as $P$ is a probability measure. \\
We firstly consider $p \in \{0,1\}$, and without lost of generality, we consider $p = 1$. \\
In this case, we can choose $\mu: = P_X,\; h_1 = h_{-1} := 1 \;$ the constant function on $X$.\\
It is then easy to calculate, that (1.5.1) and (1.5.2) will be satisfied under this choice.\\
Now suppose $p \in (0,1)$. \\
We propose that with the following quantities  (1.5.1) and (1.5.2) can be satisfied: \\
\indent $h_1 := (1-p)\, \eta, \quad h_{-1}:= p\, (1-\eta) \quad $\\
\vspace*{-1em} \\
\indent $\mu : \mathcal{A} \rightarrow [0,\infty) , A \mapsto \displaystyle{ \frac{P(Y\times A)}{p\, (1-p)} = \frac{1}{p\, (1-p)} P_X(A) }$.\\
\vspace*{-1.3em} \\
Notice that:  $h_1, h_{-1}$ are measurable since $\eta$ is measurable, and it holds that $p\, (1-p)\, \mu = P_X$. \\
Now we verify (1.5.1): \\
\indent $\displaystyle{ \int_X h_1 \, \text{d}\mu = \int_X p\, (1-\eta) \, \text{d}\mu = \int_X \frac{p\, (1-\eta)}{p\, (1-p)} \, \text{d}P_X = \frac{1}{1-p}\int_X 1-P(\{1\}|x)\, \text{d}P_X = \frac{1}{1-p} \,(1-p) = 1 }$.\\
\vspace*{-0.5em} \\
Similarly it can be calculated that $ \displaystyle{  1 = \int_X p\, (1-\eta)\,\text{d} \mu = \int_X h_{-1} \text{d} \mu }$.\vspace*{0.6em}\\
(1.5.2) should also be verified\vspace*{-0.3em}: 
\begin{multicols}{2}
\indent \indent \indent $ P( \{1\} \times A ) $ \vspace*{0.5em} \\
\indent \indent $= \displaystyle{ \int_A P(\{1\} | x )\, \text{d}  P_X }$\vspace*{0.5em}\\
\indent \indent $\displaystyle{= \int_A \eta \, \text{d} P_X }$\vspace*{0.5em}\\
\indent \indent $\displaystyle{= \int_A \eta\, p\, (1-p)\, \text{d} \mu }$\vspace*{0.5em}\\
\indent \indent $\displaystyle{= p\int_A (1-p)\, \eta \, \text{d} \mu }$\vspace*{0.5em}\\
\indent \indent $\displaystyle{= p \int_A h_1\, \text{d} \mu } $.\\
\indent \indent \indent  $ P( \{-1\} \times A ) $\vspace*{0.5em}\\
\indent \indent $= \displaystyle{ \int_A P(\{-1\} | x )\, \text{d}  P_X }$\vspace*{0.5em}\\
\indent \indent $= \displaystyle{ \int_A (1-P(\{1\} | x )  ) \, \text{d} P_X }$\vspace*{0.5em}\\
\indent \indent  $= \displaystyle{ \int_A (1- \eta)\, p\, (1-p)\, \text{d} \mu  } $\vspace*{0.5em}\\
\indent \indent $= \displaystyle{ (1-p)\int_A h_{-1} \, \text{d} \mu   }$.\\
\end{multicols}

\noindent\textsl{iii)} Find an interpretation of your findings in the spirit of Bayes's theorem. \\
\textsl{Solution:}\\
Let P an arbitrary distribution on $X \times Y$ and  $x_0 \in X$ an arbitrary element.\\ 
$(A_n)_{n \in \mathbb{N}}$ is a sequence of monotonous decreasing subsets in $X$, with $x_0 \in A_n$ for all $n \in \mathbb{N}$.\\ 
According to Bayes's theorem we'll have the equation: \[P(\{y=1\} |\; x \in A_n ) = \frac{P( \{ y = 1\} \cap \{x \in A_n \})}{P(\{x \in A_n\})} \overset{def.}{=} \frac{P(\{1\} \times A_n)}{P_X(A_n)} \]
Now, we insert (1.5.1) and (1.5.2) into this equation and receive:
\[P(\{y=1\} |\; x \in A_n ) = \frac{ p \displaystyle{\int_A h_{1} \, \text{d}\mu }}{p \displaystyle{\int_A h_{1} \, \text{d}\mu + (1-p)\displaystyle{\int_A h_{-1} \, \text{d}\mu }}} \] 
By taking the limit of $A_n$ to $\{x_0\}$, we will have (stimmt das???)
\[\eta(x_0)=P(\{1\}|x_0) = \frac{p \, h_1(x_0)}{(1-p)\,h_{-1}(x_0) + p\, h_1(x_0)}.\]
It is also the relationship we find in part i).
\end{document}